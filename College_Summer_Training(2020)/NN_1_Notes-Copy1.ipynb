{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks (ANNs)\n",
    "Artificial neural networks are one of the main tools used in machine learning. As the “neural” part of their name suggests, they are brain-inspired systems which are intended to replicate the way that we humans learn. Neural networks consist of input and output layers, as well as (in most cases) a hidden layer consisting of units that transform the input into something that the output layer can use. They are excellent tools for finding patterns which are far too complex or numerous for a human programmer to extract and teach the machine to recognize.\n",
    "\n",
    "\n",
    "For a basic idea of how a deep learning neural network learns, imagine a factory line. After the raw materials (the data set) are input, they are then passed down the conveyer belt, with each subsequent stop or layer extracting a different set of high-level features. If the network is intended to recognize an object, the first layer might analyze the brightness of its pixels.\n",
    "\n",
    "The next layer could then identify any edges in the image, based on lines of similar pixels. After this, another layer may recognize textures and shapes, and so on. By the time the fourth or fifth layer is reached, the deep learning net will have created complex feature detectors. It can figure out that certain image elements (such as a pair of eyes, a nose, and a mouth) are commonly found together.\n",
    "\n",
    "Once this is done, the researchers who have trained the network can give labels to the output, and then use backpropagation to correct any mistakes which have been made. After a while, the network can carry out its own classification tasks without needing humans to help every time.\n",
    "\n",
    "![title](3NN.GIF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you are very comfortable with sklearn library, given below is a code using MLP(Multi-layer Perceptron) classifier.\n",
    "visit \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier for documentation of MLP classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING MLP CLASSIFIER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import sklearn.datasets as ds\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preeti\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=MLPClassifier()  # creating object \n",
    "iris=ds.load_iris()  # loading dataset\n",
    "X=iris.data\n",
    "Y=iris.target\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(X,Y)\n",
    "clf.fit(xtrain,ytrain) #training neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(xtest,ytest) #obtaining score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 0, 2, 2, 0, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 1, 0, 0, 2, 1,\n",
       "       0, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 2, 0, 0, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(xtest)   # results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How exactly do NN “learn” stuff?\n",
    "\n",
    "In the same way that we learn from experience in our lives, neural networks require data to learn. In most cases, the more data that can be thrown at a neural network, the more accurate it will become. Think of it like any task you do over and over. Over time, you gradually get more efficient and make fewer mistakes.\n",
    "\n",
    "When researchers or computer scientists set out to train a neural network, they typically divide their data into three sets. First is a training set, which helps the network establish the various weights between its nodes. After this, they fine-tune it using a validation data set. Finally, they’ll use a test set to see if it can successfully turn the input into the desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do neural networks have any limitations?\n",
    "\n",
    "Biggest challenge with neural networks is the significantly large training time and the amount of computation power required to train the neural network. The biggest issue, however, is that neural networks are “black boxes,” in which the user feeds in data and receives answers. They can fine-tune the answers, but they don’t have access to the exact decision making process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK AND BACK PROPAGATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](4NN.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivativeSig(sig_out):\n",
    "    return sig_out*(1 - sig_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0,1],   # the last column is 1 for bias multiplication \n",
    "              [0,1,1],\n",
    "              [1,0,1],\n",
    "              [1,1,1]])  # trying to make neural network learn non linear decision boundary, see the graph of XOR \n",
    "Y = np.array([[0,1,1,0]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](5NN.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](7NN.jpg)\n",
    "Let's use the above mentioned error fuction for following unit:\n",
    "![title](6NN.jpg)\n",
    "\n",
    "here input has n features per training example , consequently n weights and 1 bias should be used to get :\n",
    "![title](8NN.jpg)\n",
    "\n",
    "Suppose O1 applies sigmoid on this input and gives the output as y_predicted.\n",
    "\n",
    "The sigmoid function applied is called the activation of this perceptron. It can be replaced by any other function like tanh, relu, leaky relu, or even identity function. \n",
    "\n",
    "Now simply using gradient descent to minimize error E wrt weight we do following process :\n",
    "\n",
    "![title](9NN.jpg)\n",
    "and it is given that O1 is sigmoid activation and derivative of sigmoid wrt its input is given as:\n",
    "\n",
    "![title](4NN.jpg)\n",
    "\n",
    "and since output of O1 is y_predicted we get \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](1NN.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06855632],\n",
       "       [-0.67335311],\n",
       "       [-0.46686853]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = 2 * np.random.random((3,1)) - 1       # generating random weights between -1 and 1 \n",
    "learning_rate = 0.1\n",
    "\n",
    "weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 3), (3, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.28028424,  3.25965162],\n",
       "        [-0.32451255,  2.65485483],\n",
       "        [-0.11802797,  2.86133941]]),\n",
       " array([[0.47052721, 0.94590188],\n",
       "        [0.39113578, 0.99599498],\n",
       "        [0.54047531, 0.99780853],\n",
       "        [0.45952469, 0.99984561]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for iter in range(1000):\n",
    "    output0 = X  # is basically output of 0th layer i.e the input layer hence equals to X \n",
    "    output1 = sig(np.dot(output0, weights))    # as mentioned above the output  of O1 is sigmoid applied on z which is \n",
    "                                               # dot product of input and weight matrices \n",
    "    first_term = output1 - Y                   # basically y_pred - y_act \n",
    "    second_term = derivativeSig(output1)       # output of unit O1 as mentioned above \n",
    "    first_two = first_term * second_term\n",
    "    changes = np.array([[0.0],[0.0]])\n",
    "    for i in range(2):\n",
    "        for j in range(4):\n",
    "            changes[i][0] = changes[i][0] + first_two[j][0]*output0[j][i]\n",
    "\n",
    "    # # net_change = np.dot(output0.T, first_two)\n",
    "    weights = weights - learning_rate*changes.T # updating weights  \n",
    "    \n",
    "output1 = sig(np.dot(X, weights))\n",
    "weights,output1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since above network had only one layer it wasn't able to create non-linear decision boundary and hence the results were poor.\n",
    "Now we will add one more layer and see if output changes or not. \n",
    "( You should test the above result by changing Y=[0,0,0,1] which is having a linear decision boundary. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 1, 1],\n",
       "       [1, 0, 1],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.35096536, -0.22815188,  0.61386183,  0.00161917],\n",
       "        [ 0.83115685,  0.33966994,  0.05194455, -0.16465386],\n",
       "        [ 0.82747126, -0.57970081,  0.97976553, -0.74731188]]),\n",
       " array([[ 0.33614741],\n",
       "        [-0.51443281],\n",
       "        [ 0.6471896 ],\n",
       "        [-0.33725042]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights0 = 2* np.random.random((3,4)) - 1\n",
    "weights1 = 2* np.random.random((4, 1)) - 1\n",
    "learning_rate = 0.5\n",
    "weights0,weights1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now final error depends upon the O_k, which is dependent upon the z_k, which is dependent on O_j from equation for z_k we get our required derivative. Rest is quite similar to process for single perceptron as done above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![title](10NN.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(5000):\n",
    "    layer0 = X            # Input layer \n",
    "    layer1 = sig(np.dot(layer0, weights0))  # output  of layer1 is sigmoid applied on z1 i.e. input of layer 1\n",
    "    layer2 = sig(np.dot(layer1, weights1))  # output  of layer2 is sigmoid applied on z2 i.e. input of layer 2\n",
    "    \n",
    "    l2_error = layer2 - Y                   \n",
    "    l2_delta = l2_error * derivativeSig(layer2)   # delta k \n",
    "    net_change2 = np.dot(layer1.T, l2_delta)\n",
    "\n",
    "    l1_error = l2_delta.dot(weights1.T)           # error j \n",
    "    l1_delta = l1_error  * derivativeSig(layer1)  # delta j\n",
    "    net_change1 = np.dot(layer0.T, l1_delta)\n",
    "\n",
    "    weights0 = weights0 - learning_rate*net_change1\n",
    "    weights1 = weights1 - learning_rate*net_change2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02108519],\n",
       "       [0.97051032],\n",
       "       [0.98152703],\n",
       "       [0.02625783]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer0 = X\n",
    "layer1 = sig(np.dot(layer0, weights0))\n",
    "layer2 = sig(np.dot(layer1, weights1))\n",
    "layer2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
